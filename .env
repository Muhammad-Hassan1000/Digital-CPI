# File: /.env

# Paths relative to api root folder
PIPELINE_SCRIPT_PATH = parallel_pipeline.py
DATABASE_FILE_PATH = scrape.db
METADATA_SCRIPT_PATH = data_files_analysis.py
DATA_DIR = data
SCRIPTS_DIR = scripts
# PIPELINE_SCRIPT_PATH = /app/Scraping/Final_Solution_Linux/parallel_pipeline.py
# DATABASE_FILE_PATH = /app/Scraping/Final_Solution_Linux/scrape.db
# METADATA_SCRIPT_PATH = /app/Scraping/Final_Solution_Linux/data_files_analysis.py
# DATA_DIR = /app/Scraping/Final_Solution_Linux/data
# SCRIPTS_DIR = /app/Scraping/Final_Solution_Linux/scripts

METADATA_SCRIPT_NAME = data_files_analysis.py

LOG_DIR = logs
# LOG_DIR = /app/Scraping/Final_Solution_Linux/logs
MAX_LOG_FILE_SIZE = 10
LOG_FILE_BACKUP_COUNT = 5
LOG_UTC_FORMAT = False

TASK_CONCURRENCY_LIMIT = 2
METADATA_SOURCE_ID = 0
SQLITE_TIMEOUT_SECONDS = 20
SQLITE_JOURNAL_MODE = WAL
SQLITE_SYNCHRONOUS_MODE = NORMAL


# PREFECT_API_DATABASE_CONNECTION_URL = "sqlite+aiosqlite:///root/.prefect/prefect.db?mode=wal&cache=shared"
# PREFECT_API_DATABASE_HOST = 0.0.0.0
# PREFECT_API_DATABASE_PORT = 8500
# PREFECT_API_URL = http://localhost:8500/api
# PREFECT_DEBUG_MODE = 'False'
# # PREFECT_DEFAULT_WORK_POOL_NAME = 'generic'
# PREFECT_SERVER_ANALYTICS_ENABLED = 'False'

